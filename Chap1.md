# Fundamental concepts of statistics 

## 1. Learning from data 
* Statistics as a set of tools that enable us to **learn from data**.
* start: a set of **hypotheses**  
* **prior knowledge** => strong/weak **expectations**. 
    * e.g. going to a restaurant after seeing an avg rating out of 3 reviews (weaker expectation)v.s. 300 reviews (stronger expectation)

* **machine learning**:     
    * at the interface of statistics and comp sci
    * focus on building algorithms that can learn from experience

## 2. Aggregation
* statistics provides us ways to characterize the structure of aggregates of data
* keep in mind that aggregation can go too far

## 3. Uncertainty
* Statistics provides us with the tools to characterize uncertainty, to make decisions under uncertainty, and to make predictions whose **uncertainty we can quantify**.
* statistical analysis can never “prove” a hypothesis, in the sense of demonstrating that it must be true. 
* Statistics can provide us with **evidence**, but it’s always **tentative** and subject to the uncertainty

## 4. Sampling
* The concept of aggregation implies that we can make useful insights by collapsing across data – but how much data do we need?
* the way that the study sample is obtained is critical, as it determines how broadly we can **generalize** the results.
* A fundamental insight from statistics about sampling is that while larger samples are **always better** (in terms of their ability to accurately represent the entire population), there are **diminishing returns** as the sample gets larger.
** the rate at which the benefit of larger samples decreases follows a simple mathematical rule, growing as the square root of the sample size